{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# **(Supplemental) Term Frequency - Inverse Document Frequency**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Estimated time needed: **15** minutes\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "As we've learned for non-negative matrix factorization, one application of this unsupervised dimensionality reduction is by applying it on a tf-idf matrix.\n\n## Why tf-idf?\n\nAn intuitive way to describe this is that for a given term in a document, we multiply the count of that term in the document by the how rare that term is throughout all the documents we are looking at.\n\nImagine any corpus of data. You'll probably see many many words that appear in almost all documents, such as `the`, `and`, and `so`. If you wanted to quickly analyze text to find the most important words in documents, just looking at word counts isn't good enough. Those previous words would dominate the term frequency in volume and clutter our analysis.\n\nBy performing tf-idf, we can reduce the value assigned to these words that are really common in all our documents, and increase the value of words that may appear a lot in a certain document, but not frequently in other documents.\n\n### Applications\n\nWIth a tf-idf matrix, you can succintly capture important textual information from a large group of text documents. **A corpus is defined as a large structured set of text**. It gives you an efficient representation of what the important words are to each document, and potentially how the words can relate documents together.\n\n**We will be using tf-idf matrices in the next lab!**\n\n## What is tf-idf?\n\nA tf-idf matrix is a `term frequency - inverse document frequency` matrix. Every row within this matrix will represent a document, and every column represents a term (a term could be a single word or an n-tuple of words such as *United States of America*). A tf-idf matrix is actually an augmented `term frequency`, `bag of words` or `document-term` matrix.\n\n### What is `term frequency`?\n\nA `term frequency` matrix simply counts the number of occurences of a given word within a document.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Objectives\n\nAfter completing this lab you will be able to:\n\n*   Understand what term frequency and tf-idf matrices are\n*   Explain the intuition behind both matrices and how they are calculated\n*   Apply tf-idf to a corpus of text and find the most important word in each document\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "***\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Setup\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Installing Required Libraries\n\nThe following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "The following required libraries are **not** pre-installed in the Skills Network Labs environment. **You will need to run the following cell** to install them:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import piplite\nimport micropip\nawait micropip.install(['skillsnetwork'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 41
    },
    {
      "cell_type": "markdown",
      "source": "### Importing Required Libraries\n\n*We recommend you import all required libraries in one place (here):*\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import re\nimport skillsnetwork\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n# You can also use this section to suppress warnings generated by your code:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nwarnings.filterwarnings('ignore')",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'joblib'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer, TfidfTransformer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# You can also use this section to suppress warnings generated by your code:\u001b[39;00m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/sklearn/__init__.py:87\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     84\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     90\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m     ]\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/sklearn/utils/__init__.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/sklearn/utils/_joblib.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m     _warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# joblib imports may raise DeprecationWarning on certain Python\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# versions\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m         Memory,\n\u001b[1;32m     10\u001b[0m         Parallel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         register_parallel_backend,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     24\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_parallel_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 38
    },
    {
      "cell_type": "markdown",
      "source": "## Background\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Example**\n\nLets say we have two documents with one sentence each.\n\n*   *\"We like dogs and cats\"*\n*   *\"We like cars and planes\"*\n\nIf we vectorized these two documents into a `term frequency` matrix, we would get:\n\n| doc | We | like | and | dogs | cats | cars | planes |\n| --- | -- | ---- | --- | ---- | ---- | ---- | ------ |\n| 0   | 1  | 1    | 1   | 1    | 1    | 0    | 0      |\n| 1   | 1  | 1    | 1   | 0    | 0    | 1    | 1      |\n\nWe simply count the number of words in each document. (In sklearn, they sort the words alphabetically)\n\nLets convert this into a tf-idf matrix. The value of each element is run through the following function:\n\n$\\text{idf} = (\\log \\frac{N}{|{d\\in D: t\\in d}|} + 1)$\n\n$\\text{tfidf}(t,d, D) = f\\_{t,d} \\* \\text{idf}$\n\nWhere:\n\n*   $f\\_{t,d}$ is the raw count of the term $t$ in document $d$\n*   $N$ is the total number of documents in the corpus (num of all documents)\n*   $|{d\\in D: t\\in d}|$ is the number of documents where the term $t$ appears\n*   We add 1 to the idf portion such that any word that belongs in every document is not just ignored\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Converting to a tf-idf matrix:\n\nFor document 1 the tf-idf value for `like` would be $1 \\* (\\log(\\frac{2}{2})+1) = 1$, but the tf-idf value for `dog` would be $1 \\* (\\log(\\frac{2}{1})+1) = 1.693147$\n\nIf we computed this for every element, we would have:\n\n| doc | We | like | and | dogs   | cats   | cars   | planes |\n| --- | -- | ---- | --- | ------ | ------ | ------ | ------ |\n| 0   | 1  | 1    | 1   | 1.6931 | 1.6931 | 0      | 0      |\n| 1   | 1  | 1    | 1   | 0      | 0      | 1.6931 | 1.6931 |\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Doing it in code\n\nThis is the function from sklearn that can convert a list of document strings to a term frequency matrix.\n\n```python\nCountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n```\n\nThis is the function that converts a term frequency matrix into a tf-idf matrix.\n\n```python\nTfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n```\n\nLets implement the example above using these functions!\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Corpus\nD = [\"We like dogs and cats\", \"We like cars and planes\"]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": "# Count Vectorizer creates a term frequency matrix\ncv = CountVectorizer()\ntf_mat = cv.fit_transform(D)\ntf = pd.DataFrame(tf_mat.toarray(), columns = cv.get_feature_names_out())\ntf",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'CountVectorizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Count Vectorizer creates a term frequency matrix\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cv \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m()\n\u001b[1;32m      3\u001b[0m tf_mat \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mfit_transform(D)\n\u001b[1;32m      4\u001b[0m tf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tf_mat\u001b[38;5;241m.\u001b[39mtoarray(), columns \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": "# Creating the tfidf matrix\ntfidf_trans = TfidfTransformer(smooth_idf=False)\ntfidf_mat = tfidf_trans.fit_transform(tf)\ntfidf = pd.DataFrame(tfidf_mat.toarray(), columns = tfidf_trans.get_feature_names_out())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The tf-idf matrix created above by sklearn does some normalization such that the norm (length) of each document vector (row) is 1. We can instead take the idf vector trained on our data and apply it directly to the term frequency matrix to get the non-normalized tf-idf matrix.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Non-normalized tf-idf\npd.DataFrame(tfidf_trans.idf_ * tf.to_numpy(), columns = tfidf_trans.get_feature_names_out())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Normalized tf-idf\ntfidf",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "*Note: These values are different from the ones we manually calculated as sklearn normalizes each document vector.*\n\n*I.e. $\\overrightarrow{d} \\cdot \\overrightarrow{d} = 1$*\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# d\nprint(tfidf.iloc[0,:])\n# d * d\nnp.multiply(tfidf.iloc[0,:], tfidf.iloc[0,:]).sum().round()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Exercises\n\nLets try creating a tf-idf matrix ourselves! Below we have loaded a [dataset from kaggle](https://www.kaggle.com/datasets/vivmankar/physics-vs-chemistry-vs-biology?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) of text, made up of news documents. This is an open domain dataset that is free to use.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's start by loading the data into a `pandas.DataFrame`:\n\nSince you're using `jupyterlite`, you will need to use the following method to load datasets:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "URL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/tfidf.csv'\nawait skillsnetwork.download_dataset(URL)\n# df = pd.read_json('tfidf.json')\ndf = pd.read_csv('tfidf.csv').iloc[:,1]",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Let's look at some samples rows from the dataset we loaded:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.head(5)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 1 - Count Vectorizering our text\n\nConvert this matrix of documents into a term frequency matrix. Note that this dataset has numbers, and we want to remove them for simplicity sake.\n\nUse the following function and plug it into `CountVectorizer(preprocessor=preprocess_text)` as an argument.\n\nWe also want to limit the Countvectorizer to just the top 500 words using the `max_features` argument.\n\n**Apply the `CountVectorizer` to the `df` Series and name the columns to the features from the `cv.get_feature_names_out()` function**\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Lets remove the numbers\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)\n    return text",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Your solution here\ncv = CountVectorizer(max_features = 500, preprocessor = preprocess_text)\ntf = cv.fit_transform(df)\npd.DataFrame(tf.toarray(), columns = cv.get_feature_names_out())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n    <summary>Click here for Solution</summary>\n\n```python\ncv = CountVectorizer(max_features = 500, preprocessor = preprocess_text)\ntf = cv.fit_transform(df)\npd.DataFrame(tf.toarray(), columns = cv.get_feature_names_out())\n```\n\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 2 - Applying the tf-idf transformer\n\nNow that we have a term frequency matrix, we can apply the tf-idf function to it in order to obtain a matrix where the values represent how important a certain word is to their documents.\n\n**Apply the TfidfTransformer to the `tf` matrix and name the columns to the features from `CountVectorizer.get_feature_names_out()`**\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your solution here\ntfidf_trans = TfidfTransformer()\ntfidf_mat = tfidf_trans.fit_transform(tf.toarray())\ntfidf = pd.DataFrame(tfidf_mat.toarray(), columns = cv.get_feature_names_out())\ntfidf",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n    <summary>Click here for Solution</summary>\n\n```python\ntfidf = TfidfTransformer()\ntfidf_mat = tfidf.fit_transform(tf.toarray())\npd.DataFrame(tfidf_mat.toarray(), columns = cv.get_feature_names_out())\n```\n\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Dense format matrices\n\nAs we can see above, both the term frequency and tf-idf matrices contain a lot of 0's. When dealing with very large corpus of text, or a corpus with a large amount of unique words/features, we will often store the information in a dense format. This saves us space in RAM, as well as reduces the sparsity of the original matrix.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Normal format:**\n\n| doc | apple | orange | pear |\n| --- | ----- | ------ | ---- |\n| 0   | 0.5   | 0.3    | 0    |\n| 1   | 0     | 0      | 0.4  |\n\n**Dense format:**\n\n| doc | word   | TFIDF Value |\n| --- | ------ | ----------- |\n| 0   | apple  | 0.5         |\n| 0   | orange | 0.3         |\n| 1   | pear   | 0.4         |\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In code:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tfidf",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "dense_tfidf = tfidf.stack()\ndense_tfidf[dense_tfidf != 0]",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#### Congratulations!\n\nYou've successfully completed the optional tf-idf lab, in which you learned how tf-idf matrices are created. In the next lab, you'll be using these as a starting point. Enjoy!\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Richard Ye](https://linkedin.com/in/richard-yehttps://linkedin.com/in/richard-ye?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) is a undergraduate student at the University of Toronto studying Statistics and Finance.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Other Contributors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Change Log\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n| ----------------- | ------- | ---------- | ------------------ |\n| 2022-06-03        | 0.1     | Richard Ye | Create Lab         |\n| 2022-06-21        | 0.2     | Steve Hord | QA Pass            |\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Copyright Â© 2022 IBM Corporation. All rights reserved.\n",
      "metadata": {}
    }
  ]
}